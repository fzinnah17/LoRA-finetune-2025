{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e8a1e5f5",
   "metadata": {},
   "source": [
    "### Team: Three Musketeers\n",
    "### Project: LoRA Fine-tuning Project 2\n",
    "### Script: Script.ipynb\n",
    "### Team members: Farnaz Zinnah, Chinmay Shringi, and Mohd Sarfaraz\n",
    "### Date: 04/18/2025\n",
    "### Github Link: https://github.com/fzinnah17/LoRA-finetune-2025\n",
    "### Description: This script is used to fine-tune the model on the dataset with the help of LoRA. RoBERTa-base model is used as the base model. The dataset is split into train and test sets. The train set is used to fine-tune the model and the test set is used to evaluate the performance of the fine-tuned model. The script also prints the total number of parameters in the base model and whether all the parameters are trainable. It also prints the accuracy of the fine-tuned model on the test set. LoRA is used to reduce the number of parameters in the base model. Configurations for the LoRA are also provided. The script is executed using the following command: papermill --kernel python3 script.ipynb script_executed.ipynb"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bdcc5329",
   "metadata": {
    "id": "bdcc5329",
    "papermill": {
     "duration": 0.018416,
     "end_time": "2025-04-18T12:44:01.092785",
     "exception": false,
     "start_time": "2025-04-18T12:44:01.074369",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "Install and import required libraries"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd58fd58",
   "metadata": {
    "papermill": {
     "duration": 0.016599,
     "end_time": "2025-04-18T12:44:01.128543",
     "exception": false,
     "start_time": "2025-04-18T12:44:01.111944",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Package Installation\n",
    "Installing essential libraries for transformer-based models and fine-tuning:\n",
    "- `transformers`: Hugging Face's library for state-of-the-art NLP models\n",
    "- `datasets`: Library for easily accessing and processing datasets\n",
    "- `evaluate`: Tools for model evaluation\n",
    "- `accelerate`: Enables distributed training and mixed precision\n",
    "- `peft`: Parameter-Efficient Fine-Tuning methods\n",
    "- `trl`: Transformer Reinforcement Learning\n",
    "- `bitsandbytes`: Quantization and memory optimization\n",
    "### NVIDIA GPU Monitoring\n",
    "Installing NVIDIA ML Python bindings for GPU monitoring and management\n",
    "### Model Architecture Visualization\n",
    "Installing torchinfo for visualizing PyTorch model architectures and parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "348ceed6-b684-46c3-8a32-9bb640c9a9d7",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-18T12:44:01.172764Z",
     "iopub.status.busy": "2025-04-18T12:44:01.172517Z",
     "iopub.status.idle": "2025-04-18T12:44:05.810918Z",
     "shell.execute_reply": "2025-04-18T12:44:05.810230Z"
    },
    "id": "348ceed6-b684-46c3-8a32-9bb640c9a9d7",
    "papermill": {
     "duration": 4.659626,
     "end_time": "2025-04-18T12:44:05.812232",
     "exception": false,
     "start_time": "2025-04-18T12:44:01.152606",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "!pip install transformers datasets evaluate accelerate peft trl bitsandbytes\n",
    "!pip install nvidia-ml-py3\n",
    "!pip install torchinfo\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a3cee60",
   "metadata": {
    "papermill": {
     "duration": 0.018721,
     "end_time": "2025-04-18T12:44:05.848500",
     "exception": false,
     "start_time": "2025-04-18T12:44:05.829779",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Library Imports and Setup\n",
    "\n",
    "This cell imports all necessary Python libraries for our fine-tuning project:\n",
    "\n",
    "1. **Standard Python Libraries**\n",
    "   - Basic file operations (`os`)\n",
    "   - Data serialization (`pickle`)\n",
    "   - Random number generation (`random`)\n",
    "   - Special collections (`defaultdict`)\n",
    "\n",
    "2. **Data Processing & Visualization**\n",
    "   - NumPy for numerical operations\n",
    "   - Pandas for data manipulation\n",
    "   - Matplotlib and Seaborn for creating visualizations\n",
    "\n",
    "3. **PyTorch Ecosystem**\n",
    "   - Core PyTorch functionality\n",
    "   - DataLoader for batch processing\n",
    "   - Model architecture visualization (torchinfo)\n",
    "   - Progress bars with tqdm\n",
    "\n",
    "4. **Hugging Face Tools**\n",
    "   - Dataset handling and loading\n",
    "   - RoBERTa model and tokenizer\n",
    "   - Training infrastructure (Trainer, TrainingArguments)\n",
    "   - Data collation utilities\n",
    "\n",
    "5. **PEFT (Parameter Efficient Fine-Tuning)**\n",
    "   - LoRA configuration and model adaptation\n",
    "   - Task-specific PEFT utilities\n",
    "\n",
    "6. **Evaluation Tools**\n",
    "   - Metrics calculation (accuracy, precision, recall, F1)\n",
    "   - Classification report generation\n",
    "   - Tabulated results display"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cca64f38-d8d2-4313-8295-fbbd43c2a263",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-18T12:44:05.889208Z",
     "iopub.status.busy": "2025-04-18T12:44:05.888946Z",
     "iopub.status.idle": "2025-04-18T12:44:11.750234Z",
     "shell.execute_reply": "2025-04-18T12:44:11.749645Z"
    },
    "id": "cca64f38-d8d2-4313-8295-fbbd43c2a263",
    "papermill": {
     "duration": 5.882425,
     "end_time": "2025-04-18T12:44:11.751542",
     "exception": false,
     "start_time": "2025-04-18T12:44:05.869117",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Standard libraries\n",
    "import os\n",
    "import pickle\n",
    "import random\n",
    "from collections import defaultdict\n",
    "\n",
    "# Data processing and visualization libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# PyTorch and related utilities\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "from torchinfo import summary\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Hugging Face Datasets and Transformers\n",
    "from datasets import load_dataset, Dataset, ClassLabel\n",
    "from transformers import (\n",
    "    RobertaModel,\n",
    "    RobertaTokenizer,\n",
    "    RobertaForSequenceClassification,\n",
    "    TrainingArguments,\n",
    "    Trainer,\n",
    "    DataCollatorWithPadding,\n",
    "    get_scheduler\n",
    ")\n",
    "\n",
    "# PEFT (Parameter Efficient Fine-Tuning)\n",
    "from peft import LoraConfig, get_peft_model, PeftModel, TaskType\n",
    "\n",
    "# Evaluation and metrics\n",
    "import evaluate\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score,\n",
    "    precision_score,\n",
    "    recall_score,\n",
    "    f1_score,\n",
    "    classification_report\n",
    ")\n",
    "from tabulate import tabulate\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbc4b8fa",
   "metadata": {
    "papermill": {
     "duration": 0.019009,
     "end_time": "2025-04-18T12:44:11.830386",
     "exception": false,
     "start_time": "2025-04-18T12:44:11.811377",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Data Preparation and Tokenization\n",
    "\n",
    "This cell sets up our model and prepares the AG News dataset for training:\n",
    "\n",
    "1. **Model Selection**\n",
    "   - Using RoBERTa-base as our foundation model\n",
    "   - RoBERTa is a robustly optimized BERT variant known for strong performance\n",
    "\n",
    "2. **Dataset Loading**\n",
    "   - Loading the AG News dataset from Hugging Face's datasets library\n",
    "   - Using the training split for our fine-tuning\n",
    "\n",
    "3. **Tokenization Setup**\n",
    "   - Initializing RoBERTa tokenizer\n",
    "   - The tokenizer converts text into tokens that the model can understand\n",
    "\n",
    "4. **Data Processing**\n",
    "   - Custom preprocessing function to tokenize text data\n",
    "   - Handles truncation and padding automatically\n",
    "   - Converts dataset format to be compatible with transformer training\n",
    "   - Renames 'label' column to 'labels' for compatibility with Transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21f42747-f551-40a5-a95f-7affb1eba4a3",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-18T12:44:11.870605Z",
     "iopub.status.busy": "2025-04-18T12:44:11.869809Z",
     "iopub.status.idle": "2025-04-18T12:44:16.845327Z",
     "shell.execute_reply": "2025-04-18T12:44:16.844727Z"
    },
    "id": "21f42747-f551-40a5-a95f-7affb1eba4a3",
    "papermill": {
     "duration": 4.996967,
     "end_time": "2025-04-18T12:44:16.846637",
     "exception": false,
     "start_time": "2025-04-18T12:44:11.849670",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "base_model = 'roberta-base'\n",
    "\n",
    "dataset = load_dataset('ag_news', split='train')\n",
    "tokenizer = RobertaTokenizer.from_pretrained(base_model)\n",
    "\n",
    "def preprocess(examples):\n",
    "    tokenized = tokenizer(examples['text'], truncation=True, padding=True)\n",
    "    return tokenized\n",
    "\n",
    "tokenized_dataset = dataset.map(preprocess, batched=True,  remove_columns=[\"text\"])\n",
    "tokenized_dataset = tokenized_dataset.rename_column(\"label\", \"labels\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e13319b3",
   "metadata": {
    "papermill": {
     "duration": 0.019852,
     "end_time": "2025-04-18T12:44:16.894211",
     "exception": false,
     "start_time": "2025-04-18T12:44:16.874359",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Label Processing and Data Collation Setup\n",
    "\n",
    "This cell handles the dataset's label information and sets up data batching:\n",
    "\n",
    "1. **Label Information Extraction**\n",
    "   - Determines the number of classes in the AG News dataset\n",
    "   - Extracts the class names/categories\n",
    "   - AG News is a news classification dataset with multiple categories\n",
    "\n",
    "2. **Label Mapping Creation**\n",
    "   - Creates a dictionary mapping from numeric indices to label names\n",
    "   - Essential for interpreting model predictions later\n",
    "   - Helps maintain consistency between numeric and text representations of categories\n",
    "\n",
    "3. **Data Collation Configuration**\n",
    "   - Sets up a collator that will handle batching of data\n",
    "   - Ensures proper padding of sequences within each batch\n",
    "   - Converts data to PyTorch tensors for model input\n",
    "   - Handles variable length sequences efficiently"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e07f641-bec0-43a6-8c26-510d7642916a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-18T12:44:16.933539Z",
     "iopub.status.busy": "2025-04-18T12:44:16.932800Z",
     "iopub.status.idle": "2025-04-18T12:44:16.937760Z",
     "shell.execute_reply": "2025-04-18T12:44:16.937168Z"
    },
    "id": "9e07f641-bec0-43a6-8c26-510d7642916a",
    "papermill": {
     "duration": 0.02534,
     "end_time": "2025-04-18T12:44:16.938783",
     "exception": false,
     "start_time": "2025-04-18T12:44:16.913443",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Extract the number of classess and their names\n",
    "num_labels = dataset.features['label'].num_classes\n",
    "class_names = dataset.features[\"label\"].names\n",
    "print(f\"number of labels: {num_labels}\")\n",
    "print(f\"the labels: {class_names}\")\n",
    "\n",
    "# Create an id2label mapping\n",
    "# We will need this for our classifier.\n",
    "id2label = {i: label for i, label in enumerate(class_names)}\n",
    "\n",
    "data_collator = DataCollatorWithPadding(tokenizer=tokenizer, return_tensors=\"pt\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9e24afd",
   "metadata": {
    "id": "c9e24afd",
    "papermill": {
     "duration": 0.0199,
     "end_time": "2025-04-18T12:44:16.984398",
     "exception": false,
     "start_time": "2025-04-18T12:44:16.964498",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Load Pre-trained Model\n",
    "Set up config for pretrained model and download it from hugging face"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a36ab406",
   "metadata": {
    "papermill": {
     "duration": 0.018926,
     "end_time": "2025-04-18T12:44:17.024669",
     "exception": false,
     "start_time": "2025-04-18T12:44:17.005743",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Model Initialization\n",
    "\n",
    "This cell initializes our RoBERTa model for sequence classification:\n",
    "\n",
    "1. **Model Configuration**\n",
    "   - Uses RoBERTa-base pretrained model\n",
    "   - Adapts it for sequence classification task\n",
    "   - Configures the model with our custom label mapping (id2label)\n",
    "\n",
    "2. **Architecture**\n",
    "   - Built on RoBERTa's transformer architecture\n",
    "   - Adds a classification head on top of the base model\n",
    "   - Automatically configures output dimensions for our multi-class task\n",
    "\n",
    "3. **Model Display**\n",
    "   - Prints model architecture summary\n",
    "   - Shows parameter count and layer structure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "262a8416-a59c-4ea1-95d9-0b1f81d6094c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-18T12:44:17.061983Z",
     "iopub.status.busy": "2025-04-18T12:44:17.061659Z",
     "iopub.status.idle": "2025-04-18T12:44:17.275116Z",
     "shell.execute_reply": "2025-04-18T12:44:17.274427Z"
    },
    "id": "262a8416-a59c-4ea1-95d9-0b1f81d6094c",
    "papermill": {
     "duration": 0.233535,
     "end_time": "2025-04-18T12:44:17.276311",
     "exception": false,
     "start_time": "2025-04-18T12:44:17.042776",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "model = RobertaForSequenceClassification.from_pretrained(\n",
    "    base_model,\n",
    "    id2label=id2label)\n",
    "model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f83b940",
   "metadata": {
    "papermill": {
     "duration": 0.021231,
     "end_time": "2025-04-18T12:44:17.355079",
     "exception": false,
     "start_time": "2025-04-18T12:44:17.333848",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Environment and Competition Setup\n",
    "\n",
    "This cell configures the training environment and handles competition data:\n",
    "\n",
    "1. **Constants Definition**\n",
    "   - Sets maximum trainable parameters (1M) as per project requirements\n",
    "   - Defines environment flag for Kaggle/local execution\n",
    "\n",
    "2. **Environment Detection**\n",
    "   - Automatically detects whether code is running on Kaggle or locally\n",
    "   - Provides different execution paths based on environment\n",
    "\n",
    "3. **Kaggle Integration**\n",
    "   - Handles Kaggle authentication if running on Kaggle platform\n",
    "   - Downloads competition data automatically\n",
    "   - Extracts competition files for use\n",
    "   - Project: Deep Learning Spring 2025 - Project 2\n",
    "\n",
    "4. **Error Handling**\n",
    "   - Gracefully handles non-Kaggle environments\n",
    "   - Ensures code can run both locally and on Kaggle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1cc4f99d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-18T12:44:17.397747Z",
     "iopub.status.busy": "2025-04-18T12:44:17.397081Z",
     "iopub.status.idle": "2025-04-18T12:44:17.403460Z",
     "shell.execute_reply": "2025-04-18T12:44:17.402922Z"
    },
    "papermill": {
     "duration": 0.029063,
     "end_time": "2025-04-18T12:44:17.404464",
     "exception": false,
     "start_time": "2025-04-18T12:44:17.375401",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"SETTING UP ENVIRONMENT\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Define constants\n",
    "MAX_TRAINABLE_PARAMS = 1_000_000  \n",
    "KAGGLE_ENV = False  # Set to True when running on Kaggle\n",
    "\n",
    "# Check if running on Kaggle\n",
    "try:\n",
    "    import kagglehub\n",
    "    KAGGLE_ENV = True\n",
    "    print(\"Running in Kaggle environment\")\n",
    "    \n",
    "    # Login to Kaggle\n",
    "    print(\"Logging in to Kaggle...\")\n",
    "    kagglehub.login()\n",
    "    \n",
    "    # Download competition data\n",
    "    print(\"Downloading competition data...\")\n",
    "    !kaggle competitions download -c deep-learning-spring-2025-project-2\n",
    "    !unzip -q deep-learning-spring-2025-project-2.zip\n",
    "    \n",
    "    print(\"✅ Competition data downloaded and extracted\")\n",
    "except ImportError:\n",
    "    print(\"Running in non-Kaggle environment\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "daafee0e",
   "metadata": {
    "papermill": {
     "duration": 0.020083,
     "end_time": "2025-04-18T12:44:17.443547",
     "exception": false,
     "start_time": "2025-04-18T12:44:17.423464",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Device configuration (CPU/MPS/GPU)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58fdb337",
   "metadata": {
    "papermill": {
     "duration": 0.018509,
     "end_time": "2025-04-18T12:44:17.482192",
     "exception": false,
     "start_time": "2025-04-18T12:44:17.463683",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Hardware Acceleration Setup\n",
    "\n",
    "This cell configures the computational device for model training:\n",
    "\n",
    "1. **Device Detection**\n",
    "   - Automatically detects available hardware accelerators\n",
    "   - Supports multiple platforms:\n",
    "     - Google TPU (via XLA)\n",
    "     - Apple Silicon GPU (MPS)\n",
    "     - NVIDIA GPU (CUDA)\n",
    "     - CPU (fallback option)\n",
    "\n",
    "2. **Device Assignment**\n",
    "   - Prioritizes fastest available computing device\n",
    "   - Ensures optimal training performance\n",
    "   - Handles cross-platform compatibility\n",
    "\n",
    "3. **Model Deployment**\n",
    "   - Moves the model to the selected device\n",
    "   - Prepares for GPU-accelerated training if available\n",
    "   - Confirms successful device assignment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9f7a5e8",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-18T12:44:17.527250Z",
     "iopub.status.busy": "2025-04-18T12:44:17.526739Z",
     "iopub.status.idle": "2025-04-18T12:44:18.138719Z",
     "shell.execute_reply": "2025-04-18T12:44:18.138088Z"
    },
    "papermill": {
     "duration": 0.635415,
     "end_time": "2025-04-18T12:44:18.139807",
     "exception": false,
     "start_time": "2025-04-18T12:44:17.504392",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "if \"XRT_TPU_CONFIG\" in os.environ:\n",
    "    import torch_xla.core.xla_model as xm\n",
    "    device = xm.xla_device()  # TPU\n",
    "elif torch.backends.mps.is_available():\n",
    "    device = torch.device(\"mps\")  # Apple Silicon GPU\n",
    "elif torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda\")  # NVIDIA GPU\n",
    "else:\n",
    "    device = torch.device(\"cpu\")  # CPU\n",
    "print(f\"Using device: {device} for model training and inference\")\n",
    "\n",
    "# Move model to GPU if available\n",
    "model.to(device)\n",
    "print(\"✅ Base model initialized and moved to device\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f76afc31",
   "metadata": {
    "papermill": {
     "duration": 0.019032,
     "end_time": "2025-04-18T12:44:18.216352",
     "exception": false,
     "start_time": "2025-04-18T12:44:18.197320",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Model Parameter Analysis\n",
    "\n",
    "This cell analyzes the model's parameter count and trainability:\n",
    "\n",
    "1. **Parameter Count**\n",
    "   - Calculates total number of parameters in the base model\n",
    "   - Important for understanding model complexity\n",
    "   - Helps ensure compliance with 1M trainable parameter limit\n",
    "\n",
    "2. **Trainability Check**\n",
    "   - Verifies which parameters are set for training\n",
    "   - Confirms if all parameters are trainable\n",
    "   - Essential for understanding model's training capacity\n",
    "\n",
    "3. **Project Requirements**\n",
    "   - Helps monitor compliance with project constraints\n",
    "   - Base for planning parameter-efficient fine-tuning strategies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbd0e18d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-18T12:44:18.269796Z",
     "iopub.status.busy": "2025-04-18T12:44:18.269224Z",
     "iopub.status.idle": "2025-04-18T12:44:18.274677Z",
     "shell.execute_reply": "2025-04-18T12:44:18.274106Z"
    },
    "papermill": {
     "duration": 0.034157,
     "end_time": "2025-04-18T12:44:18.275716",
     "exception": false,
     "start_time": "2025-04-18T12:44:18.241559",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "total_params = sum(p.numel() for p in model.parameters())\n",
    "print(f\"Total parameters in base model: {total_params:,}\")\n",
    "print(f\"All parameters trainable: {all(p.requires_grad for p in model.parameters())}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70a1bed9",
   "metadata": {
    "papermill": {
     "duration": 0.018706,
     "end_time": "2025-04-18T12:44:18.358795",
     "exception": false,
     "start_time": "2025-04-18T12:44:18.340089",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Data Filtering and Preprocessing\n",
    "\n",
    "This cell implements a data quality filtering strategy:\n",
    "\n",
    "1. **Text Length Filtering**\n",
    "   - Removes outlier examples based on word count\n",
    "   - Keeps articles between 20 and 70 words\n",
    "   - Ensures consistent input length for better training\n",
    "\n",
    "2. **Filtering Function**\n",
    "   - Custom function to process dataset in batches\n",
    "   - Maintains all features while filtering examples\n",
    "   - Provides detailed statistics about filtered data\n",
    "   - Preserves dataset structure and label information\n",
    "\n",
    "3. **Dataset Statistics**\n",
    "   - Tracks original and filtered dataset sizes\n",
    "   - Reports number of removed examples\n",
    "   - Helps monitor data quality and quantity\n",
    "\n",
    "4. **Quality Control**\n",
    "   - Removes very short texts that might lack context\n",
    "   - Removes very long texts that might be computational bottlenecks\n",
    "   - Improves training efficiency and model performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce8d185b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-18T12:44:18.399457Z",
     "iopub.status.busy": "2025-04-18T12:44:18.398793Z",
     "iopub.status.idle": "2025-04-18T12:44:18.415724Z",
     "shell.execute_reply": "2025-04-18T12:44:18.415169Z"
    },
    "papermill": {
     "duration": 0.038959,
     "end_time": "2025-04-18T12:44:18.416749",
     "exception": false,
     "start_time": "2025-04-18T12:44:18.377790",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"IMPLEMENTING DATA FILTERING STRATEGY\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Function to filter out very short and very long examples\n",
    "def filter_dataset(examples):\n",
    "    \"\"\"Filter out examples with very short or very long text.\"\"\"\n",
    "    text_lengths = [len(t.split()) for t in examples[\"text\"]]\n",
    "    \n",
    "    # Keep examples with word count between 15 and 150 words\n",
    "    valid_indices = [i for i, length in enumerate(text_lengths) if 20 <= length <= 70]\n",
    "    \n",
    "    # Calculate statistics for reporting\n",
    "    filtered_count = len(text_lengths) - len(valid_indices)\n",
    "    filtered_percentage = (filtered_count / len(text_lengths)) * 100\n",
    "    \n",
    "    print(f\"Filtered {filtered_count} examples ({filtered_percentage:.2f}% of the batch)\")\n",
    "    \n",
    "    return {k: [examples[k][i] for i in valid_indices] for k in examples}\n",
    "\n",
    "# Apply filtering to the training set only\n",
    "filtered_train = dataset.map(\n",
    "    filter_dataset, \n",
    "    batched=True, \n",
    "    desc=\"Filtering training data\"\n",
    ")\n",
    "\n",
    "print(f\"Original training set size: {len(dataset)}\")\n",
    "print(f\"Filtered training set size: {len(filtered_train)}\")\n",
    "print(f\"Removed {len(dataset) - len(filtered_train)} examples\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "299e6f57",
   "metadata": {
    "papermill": {
     "duration": 0.025885,
     "end_time": "2025-04-18T12:44:18.501731",
     "exception": false,
     "start_time": "2025-04-18T12:44:18.475846",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Dataset Tokenization\n",
    "\n",
    "This cell processes the filtered dataset into model-ready format:\n",
    "\n",
    "1. **Tokenization Process**\n",
    "   - Applies the previously defined preprocessing function\n",
    "   - Converts text to token IDs that the model can understand\n",
    "   - Processes data in batches for efficiency\n",
    "\n",
    "2. **Data Cleanup**\n",
    "   - Removes original text column after tokenization\n",
    "   - Keeps only the necessary tokenized features\n",
    "   - Optimizes memory usage\n",
    "\n",
    "3. **Label Standardization**\n",
    "   - Renames 'label' column to 'labels'\n",
    "   - Ensures compatibility with Hugging Face's trainer\n",
    "   - Maintains consistency with transformer library expectations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b2aa812",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-18T12:44:18.544585Z",
     "iopub.status.busy": "2025-04-18T12:44:18.543845Z",
     "iopub.status.idle": "2025-04-18T12:44:19.413725Z",
     "shell.execute_reply": "2025-04-18T12:44:19.413168Z"
    },
    "papermill": {
     "duration": 0.892733,
     "end_time": "2025-04-18T12:44:19.415015",
     "exception": false,
     "start_time": "2025-04-18T12:44:18.522282",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "filtered_tokenized = filtered_train.map(\n",
    "    preprocess, \n",
    "    batched=True,  \n",
    "    remove_columns=[\"text\"]\n",
    ")\n",
    "filtered_tokenized = filtered_tokenized.rename_column(\"label\", \"labels\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f265839d-a088-4693-8474-862641de11ed",
   "metadata": {
    "id": "f265839d-a088-4693-8474-862641de11ed",
    "papermill": {
     "duration": 0.018718,
     "end_time": "2025-04-18T12:44:19.454244",
     "exception": false,
     "start_time": "2025-04-18T12:44:19.435526",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Anything from here on can be modified"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a67f75e7",
   "metadata": {
    "papermill": {
     "duration": 0.019199,
     "end_time": "2025-04-18T12:44:19.493810",
     "exception": false,
     "start_time": "2025-04-18T12:44:19.474611",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Train-Test Split\n",
    "\n",
    "This cell divides the processed dataset into training and evaluation sets:\n",
    "\n",
    "1. **Dataset Division**\n",
    "   - Splits tokenized dataset into training and test sets\n",
    "   - Uses fixed test size of 640 examples\n",
    "   - Maintains reproducibility with seed=42\n",
    "\n",
    "2. **Split Configuration**\n",
    "   - Training set: Majority of data for model learning\n",
    "   - Evaluation set: Fixed size for consistent testing\n",
    "   - Random but deterministic splitting\n",
    "\n",
    "3. **Purpose**\n",
    "   - Enables proper model evaluation\n",
    "   - Prevents overfitting assessment\n",
    "   - Creates separate datasets for training and testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7413430-be57-482b-856e-36bd4ba799df",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-18T12:44:19.535351Z",
     "iopub.status.busy": "2025-04-18T12:44:19.534672Z",
     "iopub.status.idle": "2025-04-18T12:44:19.543337Z",
     "shell.execute_reply": "2025-04-18T12:44:19.542824Z"
    },
    "id": "e7413430-be57-482b-856e-36bd4ba799df",
    "papermill": {
     "duration": 0.030865,
     "end_time": "2025-04-18T12:44:19.544348",
     "exception": false,
     "start_time": "2025-04-18T12:44:19.513483",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Split the original training set\n",
    "split_datasets = filtered_tokenized.train_test_split(test_size=640, seed=42)\n",
    "train_dataset = split_datasets['train']\n",
    "eval_dataset = split_datasets['test']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e583b6ad",
   "metadata": {
    "papermill": {
     "duration": 0.02447,
     "end_time": "2025-04-18T12:44:19.657676",
     "exception": false,
     "start_time": "2025-04-18T12:44:19.633206",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Dataset Visualization and Analysis\n",
    "\n",
    "This cell creates comprehensive visualizations and analysis of the dataset:\n",
    "\n",
    "1. **Class Distribution Analysis**\n",
    "   - Creates bar plot of class frequencies\n",
    "   - Shows distribution across all categories\n",
    "   - Helps identify potential class imbalance\n",
    "   - Saves visualization as 'class_distribution.png'\n",
    "\n",
    "2. **Text Length Analysis**\n",
    "   - Generates histogram of text lengths\n",
    "   - Shows distribution of word counts\n",
    "   - Marks median and 95th percentile\n",
    "   - Helps understand text length patterns\n",
    "   - Saves visualization as 'text_length_distribution.png'\n",
    "\n",
    "3. **Sample Exploration**\n",
    "   - Displays random examples from each class\n",
    "   - Shows first 150 characters of each sample\n",
    "   - Provides qualitative insight into data\n",
    "   - Helps understand typical content per class\n",
    "\n",
    "4. **Visualization Features**\n",
    "   - Uses Seaborn and Matplotlib for clear plots\n",
    "   - Includes proper labels and titles\n",
    "   - Saves plots for future reference\n",
    "   - Ensures reproducible analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e8d120d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-18T12:44:19.700447Z",
     "iopub.status.busy": "2025-04-18T12:44:19.699778Z",
     "iopub.status.idle": "2025-04-18T12:44:32.461981Z",
     "shell.execute_reply": "2025-04-18T12:44:32.461410Z"
    },
    "papermill": {
     "duration": 12.784887,
     "end_time": "2025-04-18T12:44:32.462944",
     "exception": false,
     "start_time": "2025-04-18T12:44:19.678057",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"DATASET VISUALIZATION AND EXPLORATION\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# 1. Plot class distribution\n",
    "plt.figure(figsize=(10, 6))\n",
    "class_counts = dataset.to_pandas()['label'].value_counts().sort_index()\n",
    "sns.barplot(x=class_counts.index, y=class_counts.values)\n",
    "plt.xticks(class_counts.index, [class_names[i] for i in class_counts.index], rotation=45)\n",
    "plt.title('Class Distribution in Training Set')\n",
    "plt.xlabel('Class')\n",
    "plt.ylabel('Count')\n",
    "plt.tight_layout()\n",
    "plt.savefig('class_distribution.png')\n",
    "plt.show()\n",
    "\n",
    "# 2. Text length distribution\n",
    "text_lengths = [len(text.split()) for text in dataset['text']]\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.histplot(text_lengths, bins=50)\n",
    "plt.title('Text Length Distribution in Training Set')\n",
    "plt.xlabel('Number of Words')\n",
    "plt.ylabel('Count')\n",
    "plt.axvline(x=np.median(text_lengths), color='r', linestyle='--', label=f'Median: {np.median(text_lengths)}')\n",
    "plt.axvline(x=np.percentile(text_lengths, 95), color='g', linestyle='--', label=f'95th Percentile: {np.percentile(text_lengths, 95)}')\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.savefig('text_length_distribution.png')\n",
    "plt.show()\n",
    "\n",
    "# 3. Display sample examples from each class\n",
    "print(\"\\nSample examples from each class:\")\n",
    "samples_per_class = {}\n",
    "for i, class_name in enumerate(class_names):\n",
    "    class_examples = [ex for ex in dataset if ex['label'] == i]\n",
    "    samples = random.sample(class_examples, min(2, len(class_examples)))\n",
    "    samples_per_class[class_name] = samples\n",
    "    \n",
    "    print(f\"\\nClass {i}: {class_name}\")\n",
    "    for j, sample in enumerate(samples):\n",
    "        print(f\"  Example {j+1}: {sample['text'][:150]}...\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "652452e3",
   "metadata": {
    "id": "652452e3",
    "papermill": {
     "duration": 0.023567,
     "end_time": "2025-04-18T12:44:32.512782",
     "exception": false,
     "start_time": "2025-04-18T12:44:32.489215",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Setup LoRA Config\n",
    "Setup PEFT config and get peft model for finetuning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62409db4",
   "metadata": {
    "papermill": {
     "duration": 0.025286,
     "end_time": "2025-04-18T12:44:32.567379",
     "exception": false,
     "start_time": "2025-04-18T12:44:32.542093",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## LoRA (Low-Rank Adaptation) Configuration\n",
    "\n",
    "This cell sets up and tests different LoRA configurations for parameter-efficient fine-tuning:\n",
    "\n",
    "1. **Configuration Options**\n",
    "   - Minimal: Basic adaptation with query-only modifications\n",
    "   - Balanced: Moderate adaptation targeting query and value layers\n",
    "   - Comprehensive: Full adaptation of attention mechanism (query, key, value)\n",
    "\n",
    "2. **Configuration Testing**\n",
    "   - Tests each configuration against parameter limit (1M)\n",
    "   - Calculates trainable parameters\n",
    "   - Computes percentage of trainable parameters\n",
    "   - Verifies compliance with project constraints\n",
    "\n",
    "3. **Configuration Parameters**\n",
    "   - `r`: Rank of LoRA adaptation\n",
    "   - `lora_alpha`: Scaling factor for LoRA\n",
    "   - `lora_dropout`: Dropout rate for regularization\n",
    "   - `target_modules`: Which transformer components to adapt\n",
    "\n",
    "4. **Selection Logic**\n",
    "   - Filters configurations under parameter limit\n",
    "   - Selects configuration with maximum parameters within limit\n",
    "   - Ensures optimal use of available parameter budget\n",
    "   - Raises error if no valid configurations found"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd0ca0ea-86b8-47f7-8cbf-83da25685876",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-18T12:44:32.613840Z",
     "iopub.status.busy": "2025-04-18T12:44:32.613321Z",
     "iopub.status.idle": "2025-04-18T12:44:32.766647Z",
     "shell.execute_reply": "2025-04-18T12:44:32.766105Z"
    },
    "id": "bd0ca0ea-86b8-47f7-8cbf-83da25685876",
    "papermill": {
     "duration": 0.177121,
     "end_time": "2025-04-18T12:44:32.767577",
     "exception": false,
     "start_time": "2025-04-18T12:44:32.590456",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"CONFIGURING LoRA ADAPTATION\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# PEFT Config\n",
    "lora_configs = [\n",
    "    # Configuration 1: Similar to starter notebook with minimal adaptation\n",
    "    {\n",
    "        \"name\": \"minimal\",\n",
    "        \"r\": 2,\n",
    "        \"lora_alpha\": 16,\n",
    "        \"lora_dropout\": 0.05,\n",
    "        \"target_modules\": [\"query\"],\n",
    "    },\n",
    "    # Configuration 2: More extensive adaptation\n",
    "    {\n",
    "        \"name\": \"balanced\", \n",
    "        \"r\": 3,\n",
    "        \"lora_alpha\": 32,\n",
    "        \"lora_dropout\": 0.1,\n",
    "        \"target_modules\": [\"query\", \"value\"],\n",
    "    },\n",
    "    # Configuration 3: Comprehensive adaptation (might exceed parameter limit)\n",
    "    {\n",
    "        \"name\": \"comprehensive\",\n",
    "        \"r\": 4, \n",
    "        \"lora_alpha\": 96,\n",
    "        \"lora_dropout\": 0.1,\n",
    "        \"target_modules\": [\"query\", \"key\", \"value\"],\n",
    "    },\n",
    "    # Configuration 4: Focused adaptation with strong rank\n",
    "    {\n",
    "        \"name\": \"focused_strong\",\n",
    "        \"r\": 2,  \n",
    "        \"lora_alpha\": 128,  \n",
    "        \"lora_dropout\": 0.15,  \n",
    "        \"target_modules\": [\"query\", \"key\", \"value\"],\n",
    "    }\n",
    "]\n",
    "\n",
    "# Function to test if a configuration stays under the parameter limit\n",
    "def test_lora_config(config):\n",
    "    \"\"\"Test if a LoRA configuration stays under the parameter limit.\"\"\"\n",
    "    print(f\"\\nTesting configuration: {config['name']}\")\n",
    "    print(f\"  - rank (r): {config['r']}\")\n",
    "    print(f\"  - alpha: {config['lora_alpha']}\")\n",
    "    print(f\"  - target modules: {config['target_modules']}\")\n",
    "\n",
    "    # Create LoRA configuration\n",
    "    lora_config = LoraConfig(\n",
    "        r=config[\"r\"],\n",
    "        lora_alpha=config[\"lora_alpha\"],\n",
    "        lora_dropout=config[\"lora_dropout\"],\n",
    "        bias=\"none\",\n",
    "        target_modules=config[\"target_modules\"],\n",
    "        task_type=TaskType.SEQ_CLS,\n",
    "    )\n",
    "\n",
    "    # Get PEFT model\n",
    "    peft_model = get_peft_model(model, lora_config)\n",
    "\n",
    "    # Count trainable parameters\n",
    "    trainable_params = sum(p.numel() for p in peft_model.parameters() if p.requires_grad)\n",
    "    total_params = sum(p.numel() for p in peft_model.parameters())\n",
    "    trainable_percentage = 100 * trainable_params / total_params\n",
    "\n",
    "    print(f\"  - Trainable parameters: {trainable_params:,}\")\n",
    "    print(f\"  - Total parameters: {total_params:,}\")\n",
    "    print(f\"  - Trainable percentage: {trainable_percentage:.2f}%\")\n",
    "\n",
    "    # Check if under limit\n",
    "    under_limit = trainable_params < MAX_TRAINABLE_PARAMS\n",
    "    print(f\"  - Under {MAX_TRAINABLE_PARAMS:,} parameter limit: {'✅' if under_limit else '❌'}\")\n",
    "\n",
    "    return {\n",
    "        \"config\": config,\n",
    "        \"trainable_params\": trainable_params,\n",
    "        \"under_limit\": under_limit,\n",
    "        \"peft_config\": lora_config\n",
    "    }\n",
    "\n",
    "# Test all configurations\n",
    "config_results = [test_lora_config(config) for config in lora_configs]\n",
    "\n",
    "# Filter valid configurations\n",
    "valid_configs = [result for result in config_results if result[\"under_limit\"]]\n",
    "\n",
    "# Select the configuration with the most parameters (within limit)\n",
    "selected_config = max(valid_configs, key=lambda x: x[\"trainable_params\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33f17b8c",
   "metadata": {
    "papermill": {
     "duration": 0.023883,
     "end_time": "2025-04-18T12:44:32.818049",
     "exception": false,
     "start_time": "2025-04-18T12:44:32.794166",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## LoRA Model Initialization and Verification\n",
    "\n",
    "This cell finalizes the LoRA setup and performs crucial verification:\n",
    "\n",
    "1. **Selected Configuration Summary**\n",
    "   - Displays chosen LoRA configuration name\n",
    "   - Shows number of trainable parameters\n",
    "   - Provides clear overview of adaptation strategy\n",
    "\n",
    "2. **Model Creation**\n",
    "   - Initializes PEFT model with selected LoRA config\n",
    "   - Moves model to appropriate device (CPU/GPU/TPU)\n",
    "   - Applies parameter-efficient fine-tuning setup\n",
    "\n",
    "3. **Parameter Verification**\n",
    "   - Double-checks trainable parameter count\n",
    "   - Verifies total parameter count\n",
    "   - Ensures compliance with 1M parameter limit\n",
    "   - Provides detailed parameter statistics\n",
    "\n",
    "4. **Safety Checks**\n",
    "   - Asserts parameter count is within limits\n",
    "   - Confirms successful model initialization\n",
    "   - Validates hardware deployment\n",
    "   - Provides clear success/failure indicators"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8232f9cb",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-18T12:44:32.864801Z",
     "iopub.status.busy": "2025-04-18T12:44:32.864291Z",
     "iopub.status.idle": "2025-04-18T12:44:32.907087Z",
     "shell.execute_reply": "2025-04-18T12:44:32.906580Z"
    },
    "papermill": {
     "duration": 0.067148,
     "end_time": "2025-04-18T12:44:32.908028",
     "exception": false,
     "start_time": "2025-04-18T12:44:32.840880",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"-\"*50)\n",
    "print(f\"Selected LoRA configuration: {selected_config['config']['name']}\")\n",
    "print(f\"Trainable parameters: {selected_config['trainable_params']:,}\")\n",
    "print(\"-\"*50)\n",
    "\n",
    "# Create the final LoRA model with the selected configuration\n",
    "peft_config = selected_config[\"peft_config\"]\n",
    "peft_model = get_peft_model(model, peft_config)\n",
    "peft_model.to(device)\n",
    "\n",
    "# Verify parameters again\n",
    "print(\"\\nFinal model parameter verification:\")\n",
    "peft_model.print_trainable_parameters()\n",
    "\n",
    "# Double-check trainable parameters\n",
    "final_total_params = sum(p.numel() for p in peft_model.parameters())\n",
    "print(f\"Final total parameters in PEFT model: {final_total_params:,}\")\n",
    "final_trainable_params = sum(p.numel() for p in peft_model.parameters() if p.requires_grad)\n",
    "print(f\"Final trainable parameters in PEFT model: {final_trainable_params:,}\")\n",
    "assert final_trainable_params < MAX_TRAINABLE_PARAMS, f\"❌ Model exceeds parameter limit with {final_trainable_params:,} trainable parameters!\"\n",
    "print(\n",
    "    f\"✅ Final check passed! Model has {final_trainable_params:,} trainable parameters (limit: {MAX_TRAINABLE_PARAMS:,})\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12284b58",
   "metadata": {
    "id": "12284b58",
    "papermill": {
     "duration": 0.023428,
     "end_time": "2025-04-18T12:44:33.052427",
     "exception": false,
     "start_time": "2025-04-18T12:44:33.028999",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Training Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c888485",
   "metadata": {
    "papermill": {
     "duration": 0.025216,
     "end_time": "2025-04-18T12:44:33.100684",
     "exception": false,
     "start_time": "2025-04-18T12:44:33.075468",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Training and Evaluation Metrics Setup\n",
    "\n",
    "This cell configures the training process and defines evaluation metrics:\n",
    "\n",
    "1. **Optimizer Selection**\n",
    "   - Uses adamw optimizer\n",
    "   - Chosen for its adaptive learning rate properties\n",
    "\n",
    "2. **Metrics Configuration**\n",
    "   - Implements comprehensive evaluation metrics:\n",
    "     - Accuracy: Overall classification correctness\n",
    "     - Precision: Measure of prediction quality\n",
    "     - Recall: Measure of prediction completeness\n",
    "     - F1-Score: Harmonic mean of precision and recall\n",
    "\n",
    "3. **Per-Class Analysis**\n",
    "   - Calculates metrics for each class separately\n",
    "   - Enables detailed performance analysis\n",
    "   - Helps identify class-specific issues\n",
    "   - Uses weighted averages for imbalanced classes\n",
    "\n",
    "4. **Evaluation Function**\n",
    "   - Custom `compute_metrics` function for 🤗 Trainer\n",
    "   - Processes model predictions and ground truth\n",
    "   - Returns both aggregate and per-class metrics\n",
    "   - Facilitates detailed model performance tracking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ee64c43-fe38-479a-b3c5-7d939a3db4c1",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-18T12:44:33.148231Z",
     "iopub.status.busy": "2025-04-18T12:44:33.147648Z",
     "iopub.status.idle": "2025-04-18T12:44:33.153802Z",
     "shell.execute_reply": "2025-04-18T12:44:33.153292Z"
    },
    "id": "0ee64c43-fe38-479a-b3c5-7d939a3db4c1",
    "papermill": {
     "duration": 0.030539,
     "end_time": "2025-04-18T12:44:33.154700",
     "exception": false,
     "start_time": "2025-04-18T12:44:33.124161",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# To track evaluation accuracy during training\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"CONFIGURING TRAINING\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "SELECTED_OPTIMIZER = \"adamw\"  # Options: \"adamw\", \"sgd\", \"rmsprop\"\n",
    "print(f\"Selected optimizer: {SELECTED_OPTIMIZER}\")\n",
    "\n",
    "def compute_metrics(eval_pred):\n",
    "    \"\"\"Calculate multiple metrics for model evaluation.\"\"\"\n",
    "    logits, labels = eval_pred\n",
    "    predictions = np.argmax(logits, axis=-1)\n",
    "\n",
    "    # Calculate various metrics\n",
    "    accuracy = accuracy_score(labels, predictions)\n",
    "    precision = precision_score(labels, predictions, average='weighted')\n",
    "    recall = recall_score(labels, predictions, average='weighted')\n",
    "    f1 = f1_score(labels, predictions, average='weighted')\n",
    "\n",
    "    # Calculate per-class metrics\n",
    "    per_class_precision = precision_score(labels, predictions, average=None)\n",
    "    per_class_recall = recall_score(labels, predictions, average=None)\n",
    "    per_class_f1 = f1_score(labels, predictions, average=None)\n",
    "\n",
    "    # Prepare results\n",
    "    results = {\n",
    "        \"accuracy\": accuracy,\n",
    "        \"precision\": precision,\n",
    "        \"recall\": recall,\n",
    "        \"f1\": f1,\n",
    "    }\n",
    "\n",
    "    # Add per-class metrics\n",
    "    for i, class_name in id2label.items():\n",
    "        results[f\"precision_{class_name}\"] = per_class_precision[i]\n",
    "        results[f\"recall_{class_name}\"] = per_class_recall[i]\n",
    "        results[f\"f1_{class_name}\"] = per_class_f1[i]\n",
    "\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "768b4917-65de-4e55-ae7f-698e287535d4",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-18T12:44:33.210595Z",
     "iopub.status.busy": "2025-04-18T12:44:33.210287Z",
     "iopub.status.idle": "2025-04-18T12:44:33.235131Z",
     "shell.execute_reply": "2025-04-18T12:44:33.234572Z"
    },
    "id": "768b4917-65de-4e55-ae7f-698e287535d4",
    "papermill": {
     "duration": 0.054258,
     "end_time": "2025-04-18T12:44:33.236287",
     "exception": false,
     "start_time": "2025-04-18T12:44:33.182029",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./results\",\n",
    "    do_eval=True,\n",
    "    eval_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    "    save_total_limit=3,  \n",
    "    per_device_train_batch_size=48,  \n",
    "    per_device_eval_batch_size=64,\n",
    "    num_train_epochs=25,  \n",
    "    weight_decay=0.005,\n",
    "    load_best_model_at_end=True,\n",
    "    metric_for_best_model=\"accuracy\",\n",
    "    greater_is_better=True,\n",
    "    learning_rate=1e-4,\n",
    "    lr_scheduler_type=\"cosine\",\n",
    "    warmup_ratio=0.15,\n",
    "    optim=\"adamw_torch\",\n",
    "    logging_dir=\"./logs\",\n",
    "    logging_steps=50,  \n",
    "    gradient_checkpointing=False,\n",
    "    label_smoothing_factor=0.05,\n",
    "    fp16=True,\n",
    "    gradient_accumulation_steps=2,  \n",
    "    gradient_checkpointing_kwargs={'use_reentrant': True}\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8ce2f3c",
   "metadata": {
    "papermill": {
     "duration": 0.024657,
     "end_time": "2025-04-18T12:44:33.329208",
     "exception": false,
     "start_time": "2025-04-18T12:44:33.304551",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Custom Trainer Implementation\n",
    "\n",
    "This cell extends Hugging Face's Trainer class with enhanced monitoring and visualization:\n",
    "\n",
    "1. **Custom Trainer Class**\n",
    "   - Inherits from 🤗 Trainer\n",
    "   - Adds history tracking for metrics\n",
    "   - Implements custom logging functionality\n",
    "   - Enables detailed training visualization\n",
    "\n",
    "2. **Metric Tracking**\n",
    "   - Records training metrics over time:\n",
    "     - Training loss\n",
    "     - Learning rate\n",
    "     - Accuracy\n",
    "     - Custom evaluation metrics\n",
    "   - Maintains separate histories for training and evaluation\n",
    "   - Tracks steps and epochs\n",
    "\n",
    "3. **Visualization Features**\n",
    "   - Creates comprehensive training plots:\n",
    "     - Training loss curve\n",
    "     - Accuracy comparison (train vs validation)\n",
    "     - Learning rate progression\n",
    "     - Additional metric trends\n",
    "   - Saves plots as 'training_metrics.png'\n",
    "   - Provides real-time training insights\n",
    "\n",
    "4. **Reporting**\n",
    "   - Prints final training metrics\n",
    "   - Shows evaluation results\n",
    "   - Provides detailed performance summary\n",
    "   - Enables easy model performance analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d93d44bc",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-18T12:44:33.381792Z",
     "iopub.status.busy": "2025-04-18T12:44:33.381530Z",
     "iopub.status.idle": "2025-04-18T12:44:33.392484Z",
     "shell.execute_reply": "2025-04-18T12:44:33.391977Z"
    },
    "papermill": {
     "duration": 0.041466,
     "end_time": "2025-04-18T12:44:33.393403",
     "exception": false,
     "start_time": "2025-04-18T12:44:33.351937",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class CustomTrainer(Trainer):\n",
    "\n",
    "    def __init__(self, *args, **kwargs):\n",
    "        super().__init__(*args, **kwargs)\n",
    "        self.train_history = defaultdict(list)\n",
    "        self.eval_history = defaultdict(list)\n",
    "        self.step_history = []\n",
    "        self.epoch_history = []\n",
    "\n",
    "    def log(self, logs, start_time=None):\n",
    "        # Note: Added start_time parameter to match parent class\n",
    "        super().log(logs, start_time)\n",
    "\n",
    "        # Track training metrics\n",
    "        for key, value in logs.items():\n",
    "            if key.startswith(\"train_\"):\n",
    "                self.train_history[key].append(float(value))\n",
    "            elif key.startswith(\"eval_\"):\n",
    "                self.eval_history[key].append(float(value))\n",
    "\n",
    "        # Track steps and epochs\n",
    "        if \"epoch\" in logs:\n",
    "            self.epoch_history.append(float(logs[\"epoch\"]))\n",
    "        if \"step\" in logs:\n",
    "            self.step_history.append(int(logs[\"step\"]))\n",
    "\n",
    "    def plot_metrics(self):\n",
    "        \"\"\"Plot training and evaluation metrics.\"\"\"\n",
    "        if not self.step_history:\n",
    "            print(\"No training history to plot.\")\n",
    "            return\n",
    "\n",
    "        # Plot training loss\n",
    "        plt.figure(figsize=(12, 8))\n",
    "\n",
    "        if \"train_loss\" in self.train_history:\n",
    "            plt.subplot(2, 2, 1)\n",
    "            plt.plot(self.step_history,\n",
    "                     self.train_history[\"train_loss\"],\n",
    "                     label=\"Train\")\n",
    "            plt.title(\"Training Loss\")\n",
    "            plt.xlabel(\"Step\")\n",
    "            plt.ylabel(\"Loss\")\n",
    "            plt.legend()\n",
    "\n",
    "        # Plot accuracy\n",
    "        plt.subplot(2, 2, 2)\n",
    "\n",
    "        # Plot train accuracy if available\n",
    "        if \"train_accuracy\" in self.train_history:\n",
    "            plt.plot(self.step_history,\n",
    "                     self.train_history[\"train_accuracy\"],\n",
    "                     label=\"Train\")\n",
    "\n",
    "        # Plot eval accuracy\n",
    "        # eval_steps = self.step_history[::len(self.step_history)//len(self.eval_history[\"eval_accuracy\"])][:len(self.eval_history[\"eval_accuracy\"])]\n",
    "        if self.eval_history.get(\"eval_accuracy\", []):\n",
    "            step_interval = max(1, len(self.step_history) // len(self.eval_history[\"eval_accuracy\"]))\n",
    "            eval_steps = self.step_history[::step_interval][:len(self.eval_history[\"eval_accuracy\"])]\n",
    "        else:\n",
    "            eval_steps = []\n",
    "\n",
    "        plt.plot(eval_steps,\n",
    "                 self.eval_history[\"eval_accuracy\"],\n",
    "                 label=\"Validation\",\n",
    "                 marker=\"o\")\n",
    "        plt.title(\"Model Accuracy\")\n",
    "        plt.xlabel(\"Step\")\n",
    "        plt.ylabel(\"Accuracy\")\n",
    "        plt.legend()\n",
    "\n",
    "        # Plot learning rate\n",
    "        if \"learning_rate\" in self.train_history:\n",
    "            plt.subplot(2, 2, 3)\n",
    "            plt.plot(self.step_history, self.train_history[\"learning_rate\"])\n",
    "            plt.title(\"Learning Rate\")\n",
    "            plt.xlabel(\"Step\")\n",
    "            plt.ylabel(\"LR\")\n",
    "\n",
    "        # Plot eval metrics\n",
    "        plt.subplot(2, 2, 4)\n",
    "        for key, values in self.eval_history.items():\n",
    "            if key != \"eval_accuracy\" and key != \"eval_loss\" and len(\n",
    "                    values) > 0:\n",
    "                plt.plot(eval_steps, values, label=key.replace(\"eval_\", \"\"))\n",
    "\n",
    "        if len(plt.gca().get_lines()) > 0:\n",
    "            plt.title(\"Additional Metrics\")\n",
    "            plt.xlabel(\"Step\")\n",
    "            plt.ylabel(\"Score\")\n",
    "            plt.legend()\n",
    "        else:\n",
    "            plt.title(\"No Additional Metrics\")\n",
    "\n",
    "        plt.tight_layout()\n",
    "        plt.savefig('training_metrics.png')\n",
    "        plt.show()\n",
    "\n",
    "        # Print final metrics\n",
    "        print(\"\\nFinal Training Metrics:\")\n",
    "        for key, values in self.train_history.items():\n",
    "            if len(values) > 0:\n",
    "                print(f\"  {key}: {values[-1]:.4f}\")\n",
    "\n",
    "        print(\"\\nFinal Evaluation Metrics:\")\n",
    "        for key, values in self.eval_history.items():\n",
    "            if len(values) > 0:\n",
    "                print(f\"  {key}: {values[-1]:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b323044d",
   "metadata": {
    "papermill": {
     "duration": 0.022018,
     "end_time": "2025-04-18T12:44:33.487533",
     "exception": false,
     "start_time": "2025-04-18T12:44:33.465515",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Trainer Initialization\n",
    "\n",
    "This cell creates and configures the custom training instance:\n",
    "\n",
    "1. **Trainer Setup**\n",
    "   - Uses our CustomTrainer class with enhanced monitoring\n",
    "   - Connects all training components:\n",
    "     - LoRA-adapted model\n",
    "     - Training arguments\n",
    "     - Training and evaluation datasets\n",
    "     - Tokenizer\n",
    "     - Data collator\n",
    "     - Metrics computation\n",
    "\n",
    "2. **Components Integration**\n",
    "   - Binds model with datasets\n",
    "   - Incorporates custom metrics\n",
    "   - Sets up data processing pipeline\n",
    "   - Prepares for training execution\n",
    "\n",
    "3. **Training Pipeline**\n",
    "   - Establishes end-to-end training workflow\n",
    "   - Handles data batching and processing\n",
    "   - Manages model updates and evaluation\n",
    "   - Enables metric tracking and visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f1065ee",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-18T12:44:33.534513Z",
     "iopub.status.busy": "2025-04-18T12:44:33.534214Z",
     "iopub.status.idle": "2025-04-18T12:44:33.601532Z",
     "shell.execute_reply": "2025-04-18T12:44:33.600894Z"
    },
    "papermill": {
     "duration": 0.092938,
     "end_time": "2025-04-18T12:44:33.602424",
     "exception": false,
     "start_time": "2025-04-18T12:44:33.509486",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Initialize trainer\n",
    "trainer = CustomTrainer(\n",
    "    model=peft_model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=eval_dataset,\n",
    "    tokenizer=tokenizer,\n",
    "    data_collator=data_collator,\n",
    "    compute_metrics=compute_metrics,\n",
    ")\n",
    "\n",
    "print(\"✅ Training configuration complete\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b848278",
   "metadata": {
    "id": "9b848278",
    "papermill": {
     "duration": 0.022501,
     "end_time": "2025-04-18T12:44:33.650512",
     "exception": false,
     "start_time": "2025-04-18T12:44:33.628011",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Start Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98d9d57d-b57f-4acc-80fb-fc5443e75515",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-18T12:44:33.697757Z",
     "iopub.status.busy": "2025-04-18T12:44:33.697480Z",
     "iopub.status.idle": "2025-04-18T14:44:40.756578Z",
     "shell.execute_reply": "2025-04-18T14:44:40.756041Z"
    },
    "id": "98d9d57d-b57f-4acc-80fb-fc5443e75515",
    "papermill": {
     "duration": 7207.082647,
     "end_time": "2025-04-18T14:44:40.757524",
     "exception": false,
     "start_time": "2025-04-18T12:44:33.674877",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"STARTING MODEL TRAINING\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Start training\n",
    "trainer.train()\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"TRAINING COMPLETE\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Save the model\n",
    "output_dir = \"./saved_model\"\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "# Save model, configuration, and tokenizer\n",
    "peft_model.save_pretrained(output_dir)\n",
    "tokenizer.save_pretrained(output_dir)\n",
    "\n",
    "print(f\"✅ Model saved to {output_dir}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5183be7e-514f-4e64-a6f4-314a827e6be5",
   "metadata": {
    "id": "5183be7e-514f-4e64-a6f4-314a827e6be5",
    "papermill": {
     "duration": 0.022775,
     "end_time": "2025-04-18T14:44:40.805403",
     "exception": false,
     "start_time": "2025-04-18T14:44:40.782628",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Evaluate Finetuned Model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "038198cf-0953-47e7-bd47-b073d05f8378",
   "metadata": {
    "id": "038198cf-0953-47e7-bd47-b073d05f8378",
    "papermill": {
     "duration": 0.02206,
     "end_time": "2025-04-18T14:44:40.850364",
     "exception": false,
     "start_time": "2025-04-18T14:44:40.828304",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### Performing Inference on Custom Input\n",
    "Uncomment following functions for running inference on custom inputs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0add484",
   "metadata": {
    "papermill": {
     "duration": 0.021888,
     "end_time": "2025-04-18T14:44:40.896202",
     "exception": false,
     "start_time": "2025-04-18T14:44:40.874314",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Custom Text Classification Function\n",
    "\n",
    "This cell implements a function for real-world model inference:\n",
    "\n",
    "1. **Classification Function**\n",
    "   - Takes three inputs:\n",
    "     - Trained model\n",
    "     - Tokenizer\n",
    "     - Text to classify\n",
    "   - Returns predicted class label\n",
    "\n",
    "2. **Processing Pipeline**\n",
    "   - Automatically selects appropriate device (CPU/GPU)\n",
    "   - Tokenizes input text\n",
    "   - Handles padding and truncation\n",
    "   - Converts inputs to PyTorch tensors\n",
    "\n",
    "3. **Prediction Output**\n",
    "   - Processes model outputs\n",
    "   - Extracts predicted class\n",
    "   - Maps numeric prediction to human-readable label\n",
    "   - Provides detailed output including:\n",
    "     - Numeric class ID\n",
    "     - Class label\n",
    "     - Input text\n",
    "\n",
    "4. **Usage**\n",
    "   - Enables quick testing of model on new texts\n",
    "   - Provides immediate feedback on model predictions\n",
    "   - Useful for model demonstration and validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f88ad420-3f46-4eff-9d71-0ce388163062",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-18T14:44:40.941589Z",
     "iopub.status.busy": "2025-04-18T14:44:40.940956Z",
     "iopub.status.idle": "2025-04-18T14:44:40.945708Z",
     "shell.execute_reply": "2025-04-18T14:44:40.945206Z"
    },
    "id": "f88ad420-3f46-4eff-9d71-0ce388163062",
    "papermill": {
     "duration": 0.027946,
     "end_time": "2025-04-18T14:44:40.946564",
     "exception": false,
     "start_time": "2025-04-18T14:44:40.918618",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"EVALUATING MODEL ON CUSTOM INPUTS\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "def classify(model, tokenizer, text):\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    inputs = tokenizer(text, truncation=True, padding=True, return_tensors=\"pt\").to(device)\n",
    "    output = model(**inputs)\n",
    "\n",
    "    prediction = output.logits.argmax(dim=-1).item()\n",
    "\n",
    "    print(f'\\n Class: {prediction}, Label: {id2label[prediction]}, Text: {text}')\n",
    "    return id2label[prediction]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc52bb94-5e13-4943-9225-a6d7fd053579",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-18T14:44:41.003472Z",
     "iopub.status.busy": "2025-04-18T14:44:41.002963Z",
     "iopub.status.idle": "2025-04-18T14:44:41.505943Z",
     "shell.execute_reply": "2025-04-18T14:44:41.505434Z"
    },
    "id": "fc52bb94-5e13-4943-9225-a6d7fd053579",
    "papermill": {
     "duration": 0.527637,
     "end_time": "2025-04-18T14:44:41.506816",
     "exception": false,
     "start_time": "2025-04-18T14:44:40.979179",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "classify( peft_model, tokenizer, \"Kederis proclaims innocence Olympic champion Kostas Kederis today left hospital ahead of his date with IOC inquisitors claiming his ...\")\n",
    "classify( peft_model, tokenizer, \"Wall St. Bears Claw Back Into the Black (Reuters) Reuters - Short-sellers, Wall Street's dwindling\\band of ultra-cynics, are seeing green again.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68a3e276-bf8c-4403-8a48-5ef19f2beccf",
   "metadata": {
    "id": "68a3e276-bf8c-4403-8a48-5ef19f2beccf",
    "papermill": {
     "duration": 0.028061,
     "end_time": "2025-04-18T14:44:41.561423",
     "exception": false,
     "start_time": "2025-04-18T14:44:41.533362",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### Run Inference on eval_dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20598815",
   "metadata": {
    "papermill": {
     "duration": 0.021988,
     "end_time": "2025-04-18T14:44:41.605045",
     "exception": false,
     "start_time": "2025-04-18T14:44:41.583057",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Model Evaluation Implementation\n",
    "\n",
    "This cell implements two different evaluation approaches:\n",
    "\n",
    "1. **Method 1: Trainer Evaluation**\n",
    "   - Uses Hugging Face's built-in evaluation\n",
    "   - Calculates accuracy and other metrics\n",
    "   - Verifies against 80% accuracy requirement\n",
    "   - Provides immediate feedback on model performance\n",
    "\n",
    "2. **Method 2: Custom Evaluation Function**\n",
    "   - Features:\n",
    "     - Batch processing for efficiency\n",
    "     - Progress tracking with tqdm\n",
    "     - Detailed logging\n",
    "     - Support for both labeled and unlabeled data\n",
    "   - Components:\n",
    "     - DataLoader configuration\n",
    "     - Device management (CPU/GPU)\n",
    "     - Memory-efficient inference\n",
    "     - Metric computation\n",
    "\n",
    "3. **Evaluation Features**\n",
    "   - Handles both labeled and unlabeled datasets\n",
    "   - Provides detailed progress updates\n",
    "   - Implements error handling\n",
    "   - Supports custom batch sizes\n",
    "   - Returns comprehensive metrics\n",
    "\n",
    "4. **Output Details**\n",
    "   - Accuracy metrics\n",
    "   - Prediction arrays\n",
    "   - Detailed progress logs\n",
    "   - Performance warnings if needed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebbc20a2-a1c0-4cb7-b842-f52e4de61ed5",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-18T14:44:41.658171Z",
     "iopub.status.busy": "2025-04-18T14:44:41.657932Z",
     "iopub.status.idle": "2025-04-18T14:44:42.380023Z",
     "shell.execute_reply": "2025-04-18T14:44:42.379532Z"
    },
    "id": "ebbc20a2-a1c0-4cb7-b842-f52e4de61ed5",
    "papermill": {
     "duration": 0.753864,
     "end_time": "2025-04-18T14:44:42.380903",
     "exception": false,
     "start_time": "2025-04-18T14:44:41.627039",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"EVALUATING MODEL\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "eval_results = trainer.evaluate(eval_dataset)\n",
    "print(f\"Evaluation results: {eval_results}\")\n",
    "\n",
    "# Extract evaluation accuracy from the results dictionary\n",
    "final_eval_accuracy = eval_results.get(\"eval_accuracy\", 0)\n",
    "print(f\"Evaluation Accuracy (Trainer method): {final_eval_accuracy:.4f}\")\n",
    "\n",
    "if final_eval_accuracy < 0.80:\n",
    "    print(f\"⚠️ Warning: The model accuracy ({final_eval_accuracy:.4f}) is below the minimum requirement of 80%\")\n",
    "else:\n",
    "    print(f\"✅ Model meets the minimum accuracy requirement!\")\n",
    "\n",
    "from torch.utils.data import DataLoader\n",
    "import evaluate\n",
    "from tqdm import tqdm\n",
    "\n",
    "def evaluate_model(inference_model, dataset, labelled=True, batch_size=8, data_collator=None):\n",
    "    \"\"\"\n",
    "    Evaluate a PEFT model on a dataset.\n",
    "    \n",
    "    Args:\n",
    "        inference_model: The model to evaluate.\n",
    "        dataset: The dataset (Hugging Face Dataset) to run inference on.\n",
    "        labelled (bool): If True, the dataset includes labels and metrics will be computed.\n",
    "                         If False, only predictions will be returned.\n",
    "        batch_size (int): Batch size for inference.\n",
    "        data_collator: Function to collate batches.\n",
    "    \n",
    "    Returns:\n",
    "        If labelled is True, returns a tuple (metrics, predictions)\n",
    "        If labelled is False, returns the predictions.\n",
    "    \"\"\"\n",
    "    print(f\"Starting evaluation with batch size {batch_size}...\")\n",
    "    print(f\"Dataset size: {len(dataset)} examples\")\n",
    "    print(f\"Dataset format: {dataset.format}\")\n",
    "    print(f\"Evaluation mode: {'Labelled' if labelled else 'Unlabelled'}\")\n",
    "    \n",
    "    # Create the DataLoader\n",
    "    eval_dataloader = DataLoader(dataset, batch_size=batch_size, collate_fn=data_collator)\n",
    "    print(f\"Created DataLoader with {len(eval_dataloader)} batches\")\n",
    "    \n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    print(f\"Using device: {device}\")\n",
    "    \n",
    "    inference_model.to(device)\n",
    "    inference_model.eval()\n",
    "    print(\"Model set to evaluation mode\")\n",
    "    \n",
    "    all_predictions = []\n",
    "    if labelled:\n",
    "        print(\"Loading accuracy metric for evaluation\")\n",
    "        metric = evaluate.load('accuracy')\n",
    "    \n",
    "    # Loop over the DataLoader\n",
    "    print(\"Starting inference loop...\")\n",
    "    \n",
    "    for batch_idx, batch in enumerate(tqdm(eval_dataloader, desc=\"Running evaluation\")):\n",
    "        # Print batch info occasionally\n",
    "        if batch_idx % 10 == 0:\n",
    "            print(f\"Processing batch {batch_idx}/{len(eval_dataloader)}\")\n",
    "            \n",
    "        # Move each tensor in the batch to the device\n",
    "        batch = {k: v.to(device) for k, v in batch.items()}\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            outputs = inference_model(**batch)\n",
    "        \n",
    "        predictions = outputs.logits.argmax(dim=-1)\n",
    "        all_predictions.append(predictions.cpu())\n",
    "        \n",
    "        if labelled:\n",
    "            references = batch[\"labels\"]\n",
    "            metric.add_batch(\n",
    "                predictions=predictions.cpu().numpy(),\n",
    "                references=references.cpu().numpy()\n",
    "            )\n",
    "    \n",
    "    # Concatenate predictions from all batches\n",
    "    all_predictions = torch.cat(all_predictions, dim=0)\n",
    "    print(f\"Completed inference. Total predictions: {len(all_predictions)}\")\n",
    "    \n",
    "    if labelled:\n",
    "        eval_metric = metric.compute()\n",
    "        print(\"Evaluation Metric:\", eval_metric)\n",
    "        return eval_metric, all_predictions\n",
    "    else:\n",
    "        return all_predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0a52645",
   "metadata": {
    "papermill": {
     "duration": 0.023236,
     "end_time": "2025-04-18T14:44:42.427450",
     "exception": false,
     "start_time": "2025-04-18T14:44:42.404214",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Check evaluation accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85450a83",
   "metadata": {
    "papermill": {
     "duration": 0.023217,
     "end_time": "2025-04-18T14:44:42.483141",
     "exception": false,
     "start_time": "2025-04-18T14:44:42.459924",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Evaluation Methods Comparison\n",
    "\n",
    "This cell compares different evaluation approaches to ensure result consistency:\n",
    "\n",
    "1. **Evaluation**\n",
    "   - Runs evaluation using specified method\n",
    "   - Uses batch size of 8\n",
    "   - Processes entire evaluation dataset\n",
    "   - Returns both metrics and predictions\n",
    "\n",
    "2. **Results Comparison**\n",
    "   - Compares two evaluation methods:\n",
    "     - Hugging Face Trainer evaluation\n",
    "     - Custom evaluation\n",
    "   - Calculates accuracy difference\n",
    "   - Checks for result consistency\n",
    "\n",
    "3. **Validation Checks**\n",
    "   - Ensures evaluation reliability\n",
    "   - Flags significant differences (>0.01)\n",
    "   - Provides clear success/warning indicators\n",
    "   - Helps identify potential evaluation issues\n",
    "\n",
    "4. **Output Details**\n",
    "   - Shows accuracies from both methods\n",
    "   - Displays numerical difference\n",
    "   - Provides clear pass/fail indication\n",
    "   - Ensures evaluation transparency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "809635a6-a2c7-4d09-8d60-ababd1815003",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-18T14:44:42.530537Z",
     "iopub.status.busy": "2025-04-18T14:44:42.530304Z",
     "iopub.status.idle": "2025-04-18T14:44:44.973252Z",
     "shell.execute_reply": "2025-04-18T14:44:44.972731Z"
    },
    "id": "809635a6-a2c7-4d09-8d60-ababd1815003",
    "papermill": {
     "duration": 2.467568,
     "end_time": "2025-04-18T14:44:44.974147",
     "exception": false,
     "start_time": "2025-04-18T14:44:42.506579",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "print(\"# Check evaluation accuracy\")\n",
    "eval_metric, predictions = evaluate_model(peft_model, eval_dataset, True, 8, data_collator)\n",
    "\n",
    "# Compare results from both methods\n",
    "print(\"\\n\" + \"-\"*50)\n",
    "print(\"COMPARISON OF EVALUATION METHODS\")\n",
    "print(\"-\"*50)\n",
    "print(f\"Trainer method accuracy: {final_eval_accuracy:.4f}\")\n",
    "print(f\"Eval accuracy: {eval_metric['accuracy']:.4f}\")\n",
    "accuracy_diff = abs(final_eval_accuracy - eval_metric['accuracy'])\n",
    "print(f\"Difference: {accuracy_diff:.4f}\")\n",
    "\n",
    "if accuracy_diff < 0.01:\n",
    "    print(f\"✅ Both methods yield similar results (difference < 0.01)\")\n",
    "else:\n",
    "    print(f\"⚠️ Methods show some difference in results with {accuracy_diff:.4f} difference\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "181d9627",
   "metadata": {
    "papermill": {
     "duration": 0.023782,
     "end_time": "2025-04-18T14:44:45.081601",
     "exception": false,
     "start_time": "2025-04-18T14:44:45.057819",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Detailed Evaluation Metrics and Visualizations\n",
    "\n",
    "This cell provides comprehensive model performance analysis:\n",
    "\n",
    "1. **Prediction Generation**\n",
    "   - Uses trainer to generate predictions\n",
    "   - Processes entire evaluation dataset\n",
    "   - Converts logits to class predictions\n",
    "   - Compares with true labels\n",
    "\n",
    "2. **Statistical Metrics**\n",
    "   - Confusion Matrix\n",
    "     - Shows prediction distribution\n",
    "     - Highlights model strengths/weaknesses\n",
    "   - Classification Report\n",
    "     - Per-class precision\n",
    "     - Per-class recall\n",
    "     - Per-class F1-score\n",
    "     - Overall accuracy\n",
    "\n",
    "3. **Visualizations**\n",
    "   - Raw Confusion Matrix\n",
    "     - Shows absolute numbers\n",
    "     - Saved as 'eval_confusion_matrix.png'\n",
    "   - Normalized Confusion Matrix\n",
    "     - Shows percentage distribution\n",
    "     - Saved as 'eval_normalized_confusion_matrix.png'\n",
    "   - Per-Class Accuracy\n",
    "     - Bar plot of class-wise performance\n",
    "     - Saved as 'eval_per_class_accuracy.png'\n",
    "\n",
    "4. **Output Files**\n",
    "   - Generates three visualization files\n",
    "   - Provides both numerical and visual insights\n",
    "   - Enables detailed performance analysis\n",
    "   - Facilitates result documentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce9f8698",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-18T14:44:45.130786Z",
     "iopub.status.busy": "2025-04-18T14:44:45.130278Z",
     "iopub.status.idle": "2025-04-18T14:44:46.786972Z",
     "shell.execute_reply": "2025-04-18T14:44:46.786470Z"
    },
    "papermill": {
     "duration": 1.682229,
     "end_time": "2025-04-18T14:44:46.787820",
     "exception": false,
     "start_time": "2025-04-18T14:44:45.105591",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"DETAILED METRICS AND VISUALIZATION\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Generate predictions on the eval set\n",
    "print(\"Generating detailed predictions using Trainer...\")\n",
    "predictions_output = trainer.predict(eval_dataset)\n",
    "y_true = predictions_output.label_ids\n",
    "y_pred = np.argmax(predictions_output.predictions, axis=1)\n",
    "\n",
    "# Compute confusion matrix and classification report\n",
    "from sklearn.metrics import confusion_matrix, classification_report, accuracy_score\n",
    "\n",
    "cm = confusion_matrix(y_true, y_pred)\n",
    "print(\"Confusion Matrix:\")\n",
    "print(cm)\n",
    "\n",
    "report = classification_report(y_true, y_pred, target_names=class_names)\n",
    "print(\"\\nDetailed Classification Report:\")\n",
    "print(report)\n",
    "\n",
    "# Create visualizations\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# 1. Plot confusion matrix\n",
    "plt.figure(figsize=(10, 8))\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=class_names, yticklabels=class_names)\n",
    "plt.title('Confusion Matrix on Evaluation Set')\n",
    "plt.ylabel('True Label')\n",
    "plt.xlabel('Predicted Label')\n",
    "plt.tight_layout()\n",
    "plt.savefig('eval_confusion_matrix.png')\n",
    "plt.show()\n",
    "\n",
    "# 2. Plot normalized confusion matrix\n",
    "plt.figure(figsize=(10, 8))\n",
    "cm_norm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
    "sns.heatmap(cm_norm, annot=True, fmt='.2f', cmap='Blues', xticklabels=class_names, yticklabels=class_names)\n",
    "plt.title('Normalized Confusion Matrix on Evaluation Set')\n",
    "plt.ylabel('True Label')\n",
    "plt.xlabel('Predicted Label')\n",
    "plt.tight_layout()\n",
    "plt.savefig('eval_normalized_confusion_matrix.png')\n",
    "plt.show()\n",
    "\n",
    "# 3. Plot per-class accuracy\n",
    "per_class_accuracy = cm_norm.diagonal()\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.barplot(x=list(range(len(class_names))), y=per_class_accuracy)\n",
    "plt.xticks(range(len(class_names)), class_names, rotation=45)\n",
    "plt.title('Per-Class Accuracy on Evaluation Set')\n",
    "plt.xlabel('Class')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.tight_layout()\n",
    "plt.savefig('eval_per_class_accuracy.png')\n",
    "plt.show()\n",
    "\n",
    "print(\"\\n✅ Evaluation complete with detailed metrics and visualizations\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9afb0018",
   "metadata": {
    "papermill": {
     "duration": 0.03983,
     "end_time": "2025-04-18T14:44:46.920302",
     "exception": false,
     "start_time": "2025-04-18T14:44:46.880472",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Data Loading Utility Function\n",
    "\n",
    "This cell defines a helper function for loading pickle files:\n",
    "\n",
    "1. **Function Purpose**\n",
    "   - Loads serialized data from pickle files\n",
    "   - Handles binary file operations\n",
    "   - Uses proper encoding for compatibility\n",
    "   - Provides error handling through context manager\n",
    "\n",
    "2. **Implementation Details**\n",
    "   - Function: `unpickle`\n",
    "   - Input: File path to pickle file\n",
    "   - Output: Dictionary of loaded data\n",
    "   - Uses 'bytes' encoding for compatibility\n",
    "\n",
    "3. **Usage Context**\n",
    "   - Safely loads serialized data\n",
    "   - Handles file operations efficiently\n",
    "   - Automatically closes file resources\n",
    "   - Provides progress feedback\n",
    "\n",
    "4. **Best Practices**\n",
    "   - Uses context manager (`with` statement)\n",
    "   - Includes function documentation\n",
    "   - Provides clear parameter descriptions\n",
    "   - Implements proper error handling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4ca81d5",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-18T14:44:46.980410Z",
     "iopub.status.busy": "2025-04-18T14:44:46.979851Z",
     "iopub.status.idle": "2025-04-18T14:44:46.983577Z",
     "shell.execute_reply": "2025-04-18T14:44:46.983013Z"
    },
    "papermill": {
     "duration": 0.033958,
     "end_time": "2025-04-18T14:44:46.984422",
     "exception": false,
     "start_time": "2025-04-18T14:44:46.950464",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def unpickle(file):\n",
    "    \"\"\"\n",
    "    Load data from pickle files efficiently.\n",
    "    \n",
    "    Args:\n",
    "        file: Path to pickle file\n",
    "    Returns:\n",
    "        Dictionary containing batch data\n",
    "    \"\"\"\n",
    "    print(f\"Loading file: {file}\")\n",
    "    with open(file, 'rb') as fo:\n",
    "        data_dict = pickle.load(fo, encoding='bytes')\n",
    "    return data_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75f39087-f2bb-49d3-9fe1-0d812fb30203",
   "metadata": {
    "id": "75f39087-f2bb-49d3-9fe1-0d812fb30203",
    "papermill": {
     "duration": 0.028523,
     "end_time": "2025-04-18T14:44:47.048973",
     "exception": false,
     "start_time": "2025-04-18T14:44:47.020450",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### Run Inference on unlabelled dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81a27c36",
   "metadata": {
    "papermill": {
     "duration": 0.029114,
     "end_time": "2025-04-18T14:44:47.114741",
     "exception": false,
     "start_time": "2025-04-18T14:44:47.085627",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Unlabeled Test Data Processing and Prediction\n",
    "\n",
    "This cell handles the processing and prediction of unlabeled test data:\n",
    "\n",
    "1. **Data Loading Strategy**\n",
    "   - Supports multiple environments:\n",
    "     - Kaggle-specific paths\n",
    "     - Local development paths\n",
    "     - Flexible directory structure\n",
    "   - Implements robust file finding logic\n",
    "   - Handles multiple possible file locations\n",
    "\n",
    "2. **Data Processing Pipeline**\n",
    "   - Loads pickle file data\n",
    "   - Converts to Hugging Face Dataset format\n",
    "   - Applies tokenization\n",
    "   - Prepares for model inference\n",
    "   - Handles both Pandas and custom unpickling\n",
    "\n",
    "3. **Prediction Generation**\n",
    "   - Processes test data in batches\n",
    "   - Uses optimized batch size (32)\n",
    "   - Generates class predictions\n",
    "   - Creates submission-ready format\n",
    "\n",
    "4. **Output Generation**\n",
    "   - Creates submission CSV file\n",
    "   - Includes prediction statistics\n",
    "   - Shows class distribution\n",
    "   - Provides detailed error handling\n",
    "   - Saves results as 'submission.csv'\n",
    "\n",
    "5. **Error Handling**\n",
    "   - Robust exception management\n",
    "   - Clear error reporting\n",
    "   - Graceful failure handling\n",
    "   - Informative status messages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2af62541-2c33-4f16-bb1c-cc969c715cd7",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-18T14:44:47.179183Z",
     "iopub.status.busy": "2025-04-18T14:44:47.178938Z",
     "iopub.status.idle": "2025-04-18T14:45:00.140683Z",
     "shell.execute_reply": "2025-04-18T14:45:00.140168Z"
    },
    "id": "2af62541-2c33-4f16-bb1c-cc969c715cd7",
    "papermill": {
     "duration": 12.998146,
     "end_time": "2025-04-18T14:45:00.141650",
     "exception": false,
     "start_time": "2025-04-18T14:44:47.143504",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "try:\n",
    "    if KAGGLE_ENV:\n",
    "        # Kaggle-specific paths\n",
    "        possible_paths = [\n",
    "            \"/kaggle/input/deep-learning-spring-2025-project-2/test_unlabelled.pkl\",  # Kaggle input path\n",
    "            \"/kaggle/working/test_unlabelled.pkl\",  # Kaggle working directory\n",
    "            \"test_unlabelled.pkl\",  # Current directory\n",
    "            \"./test_unlabelled.pkl\",  # Explicit current directory\n",
    "        ]\n",
    "        \n",
    "        # Try each path until we find the file\n",
    "        test_path = None\n",
    "        for path in possible_paths:\n",
    "            if os.path.exists(path):\n",
    "                test_path = path\n",
    "                break\n",
    "        \n",
    "        if test_path is None:\n",
    "            raise FileNotFoundError(\"Could not find test_unlabelled.pkl in any of the expected Kaggle locations\")\n",
    "        \n",
    "        print(f\"Loading unlabelled test data from {test_path}\")\n",
    "        \n",
    "        # Load the unlabelled test data using pandas\n",
    "        unlabelled_dataset = pd.read_pickle(test_path)\n",
    "    else:\n",
    "        # Non-Kaggle environment - look in data subdirectory\n",
    "        data_dir = \"./data\"\n",
    "        if not os.path.exists(data_dir):\n",
    "            # Try creating the directory if it doesn't exist\n",
    "            try:\n",
    "                os.makedirs(data_dir, exist_ok=True)\n",
    "                print(f\"Created data directory at {data_dir}\")\n",
    "            except:\n",
    "                print(f\"Could not create {data_dir}, trying other locations\")\n",
    "                \n",
    "            # Try one level up\n",
    "            data_dir = \"../data\"\n",
    "            if not os.path.exists(data_dir):\n",
    "                raise FileNotFoundError(\"Could not find or create the data directory\")\n",
    "        \n",
    "        test_path = os.path.join(data_dir, \"test_unlabelled.pkl\")\n",
    "        if not os.path.exists(test_path):\n",
    "            # Check current directory as last resort\n",
    "            if os.path.exists(\"test_unlabelled.pkl\"):\n",
    "                test_path = \"test_unlabelled.pkl\"\n",
    "            else:\n",
    "                raise FileNotFoundError(f\"Could not find test_unlabelled.pkl in {data_dir} or current directory\")\n",
    "        \n",
    "        print(f\"Loading unlabelled test data from {test_path}\")\n",
    "        \n",
    "        unlabelled_dataset = unpickle(test_path)\n",
    "    \n",
    "    print(f\"✅ Loaded unlabelled test dataset with {len(unlabelled_dataset['text'])} examples\")\n",
    "    \n",
    "    # Load the unlabelled test data\n",
    "    unlabelled_dataset = pd.read_pickle(test_path)\n",
    "    \n",
    "    print(f\"✅ Loaded unlabelled test dataset with {len(unlabelled_dataset['text'])} examples\")\n",
    "    \n",
    "    # Convert to HuggingFace Dataset\n",
    "    test_dataset = Dataset.from_dict({\"text\": unlabelled_dataset[\"text\"]})\n",
    "    \n",
    "    # Tokenize test data\n",
    "    tokenized_unlabelled = test_dataset.map(preprocess, batched=True, desc=\"Tokenizing unlabelled data\")\n",
    "    tokenized_unlabelled.set_format(\"torch\", columns=[\"input_ids\", \"attention_mask\"])\n",
    "    \n",
    "    # Get predictions\n",
    "    print(\"Generating predictions for unlabelled data...\")\n",
    "\n",
    "    predictions = evaluate_model(peft_model, tokenized_unlabelled, False, 32, data_collator)\n",
    "    \n",
    "    # Create submission file\n",
    "    df = pd.DataFrame({\n",
    "        \"ID\": range(len(predictions)),\n",
    "        \"label\": predictions.numpy()\n",
    "    })\n",
    "    \n",
    "    # Save to CSV\n",
    "    submission_path = \"submission.csv\"\n",
    "    df.to_csv(submission_path, index=False)\n",
    "    print(f\"✅ Predictions saved to {submission_path}\")\n",
    "    \n",
    "    # Show prediction summary\n",
    "    print(\"\\nPrediction summary:\")\n",
    "    print(f\"Total predictions: {len(predictions)}\")\n",
    "    print(f\"Unique class predictions: {torch.unique(predictions).tolist()}\")\n",
    "    value_counts = pd.Series(predictions.numpy()).value_counts().sort_index()\n",
    "    print(f\"Class distribution:\\n{value_counts}\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"⚠️ Could not process unlabelled test data: {str(e)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e19b91c6",
   "metadata": {
    "papermill": {
     "duration": 0.033879,
     "end_time": "2025-04-18T14:45:00.211263",
     "exception": false,
     "start_time": "2025-04-18T14:45:00.177384",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Final Report - Project Requirements Verification"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2d73b5c",
   "metadata": {
    "papermill": {
     "duration": 0.033153,
     "end_time": "2025-04-18T14:45:00.278388",
     "exception": false,
     "start_time": "2025-04-18T14:45:00.245235",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Final Project Report and Requirements Verification\n",
    "\n",
    "This cell generates a comprehensive final report of the project:\n",
    "\n",
    "1. **Requirements Checklist**\n",
    "   - Modified BERT architecture (RoBERTa with LoRA)\n",
    "   - Parameter count verification (<1M limit)\n",
    "   - LoRA configuration experiments\n",
    "   - Optimizer selection\n",
    "   - Data filtering implementation\n",
    "   - Learning rate scheduling\n",
    "   - Evaluation metrics\n",
    "   - Accuracy target (≥80%)\n",
    "\n",
    "2. **Architecture Details**\n",
    "   - Base model specifications\n",
    "   - LoRA configuration\n",
    "     - Rank (r)\n",
    "     - Alpha value\n",
    "     - Target modules\n",
    "   - Parameter counts\n",
    "     - Total parameters\n",
    "     - Trainable parameters\n",
    "     - Parameter efficiency ratio\n",
    "\n",
    "3. **Training Configuration**\n",
    "   - Optimizer choice\n",
    "   - Learning rate settings\n",
    "   - Batch size\n",
    "   - Number of epochs\n",
    "   - Weight decay\n",
    "   - Other hyperparameters\n",
    "\n",
    "4. **Final Results**\n",
    "   - Evaluation metrics\n",
    "   - Performance statistics\n",
    "   - Project completion status\n",
    "   - Requirements fulfillment verification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e60991d3-38b1-4657-8854-408ce66f6b84",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-18T14:45:00.348123Z",
     "iopub.status.busy": "2025-04-18T14:45:00.347842Z",
     "iopub.status.idle": "2025-04-18T14:45:00.355321Z",
     "shell.execute_reply": "2025-04-18T14:45:00.354811Z"
    },
    "id": "e60991d3-38b1-4657-8854-408ce66f6b84",
    "papermill": {
     "duration": 0.043545,
     "end_time": "2025-04-18T14:45:00.356277",
     "exception": false,
     "start_time": "2025-04-18T14:45:00.312732",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"FINAL REPORT - PROJECT REQUIREMENTS VERIFICATION\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "requirements = [\n",
    "    (\"1. Modified BERT architecture\", \"✅ Used RoBERTa base model with LoRA adaptation\"),\n",
    "    (\"2. Parameter count < 1M\", f\"✅ Model has {final_trainable_params:,} trainable parameters\"),\n",
    "    (\"3. Experimented with LoRA settings\", f\"✅ Tested {len(lora_configs)} different LoRA configurations\"),\n",
    "    (\"4. Experimented with optimizer\", f\"✅ Used {SELECTED_OPTIMIZER} optimizer\"),\n",
    "    (\"5. Implemented data filtering\", \"✅ Filtered out examples based on text length\"),\n",
    "    (\"6. Used learning rate scheduling\", \"✅ Implemented linear LR schedule with warmup\"),\n",
    "    (\"7. Comprehensive evaluation\", \"✅ Calculated accuracy, precision, recall, and F1 metrics\"),\n",
    "    (\"8. Target accuracy ≥ 80%\", f\"✅ Achieved {final_eval_accuracy:.2%} accuracy on eval set\"),\n",
    "]\n",
    "\n",
    "for req, status in requirements:\n",
    "    print(f\"  {req}: {status}\")\n",
    "\n",
    "print(\"\\nModel Architecture Summary:\")\n",
    "print(f\"  Base model: RoBERTa\")\n",
    "print(f\"  LoRA rank (r): {selected_config['config']['r']}\")\n",
    "print(f\"  LoRA alpha: {selected_config['config']['lora_alpha']}\")\n",
    "print(f\"  Target modules: {selected_config['config']['target_modules']}\")\n",
    "print(f\"  Total parameters in final model: {final_total_params:,}\")\n",
    "print(f\"  Trainable parameters: {final_trainable_params:,} ({final_trainable_params/total_params:.2%} of total)\")\n",
    "\n",
    "print(\"\\nTraining Summary:\")\n",
    "print(f\"  Optimizer: {SELECTED_OPTIMIZER}\")\n",
    "print(f\"  Learning rate: {training_args.learning_rate}\")\n",
    "print(f\"  Batch size: {training_args.per_device_train_batch_size}\")\n",
    "print(f\"  Epochs: {training_args.num_train_epochs}\")\n",
    "print(f\"  Weight decay: {training_args.weight_decay}\")\n",
    "\n",
    "print(\"\\nFinal Test Results:\")\n",
    "for metric, value in eval_results.items():\n",
    "    print(f\"  {metric}: {value:.4f}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"PROJECT COMPLETE\")\n",
    "print(\"=\"*80)"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 7261.848526,
   "end_time": "2025-04-18T14:45:01.709600",
   "environment_variables": {},
   "exception": null,
   "input_path": "chinmay_with_new_filter.ipynb",
   "output_path": "results/chinmay_with_new_filter_executed.ipynb",
   "parameters": {},
   "start_time": "2025-04-18T12:43:59.861074",
   "version": "2.6.0"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {
     "001ffcbf920e470e9bd787463cea1d54": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HBoxModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HBoxModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "HBoxView",
       "box_style": "",
       "children": [
        "IPY_MODEL_286393660bfb41cba708347763595d72",
        "IPY_MODEL_7bee36e58f554d5a9d99f3f81a3e3c0c",
        "IPY_MODEL_f1ffcc7247d642d886d9892eccc1b531"
       ],
       "layout": "IPY_MODEL_a39b7bd6e90041aeb0bc7e27fa29a448",
       "tabbable": null,
       "tooltip": null
      }
     },
     "04e4eadef1114d8ea0ede08e69718d27": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "ProgressStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "ProgressStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "StyleView",
       "bar_color": null,
       "description_width": ""
      }
     },
     "20646b922e054a0d835ca9cc996544aa": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "286393660bfb41cba708347763595d72": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_allow_html": false,
       "layout": "IPY_MODEL_ddcff641e2a44415a18a5209bd9eff60",
       "placeholder": "​",
       "style": "IPY_MODEL_fd070c94aedd4c57a942be665a66527f",
       "tabbable": null,
       "tooltip": null,
       "value": "Tokenizing unlabelled data: 100%"
      }
     },
     "28b7f0b6e88e4226a127ae93c0eda102": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "7bee36e58f554d5a9d99f3f81a3e3c0c": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "FloatProgressModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "FloatProgressModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "ProgressView",
       "bar_style": "success",
       "description": "",
       "description_allow_html": false,
       "layout": "IPY_MODEL_28b7f0b6e88e4226a127ae93c0eda102",
       "max": 8000,
       "min": 0,
       "orientation": "horizontal",
       "style": "IPY_MODEL_04e4eadef1114d8ea0ede08e69718d27",
       "tabbable": null,
       "tooltip": null,
       "value": 8000
      }
     },
     "7d8e7b237e5c4b0db9351d10b3e3fba7": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "StyleView",
       "background": null,
       "description_width": "",
       "font_size": null,
       "text_color": null
      }
     },
     "a39b7bd6e90041aeb0bc7e27fa29a448": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "ddcff641e2a44415a18a5209bd9eff60": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "f1ffcc7247d642d886d9892eccc1b531": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_allow_html": false,
       "layout": "IPY_MODEL_20646b922e054a0d835ca9cc996544aa",
       "placeholder": "​",
       "style": "IPY_MODEL_7d8e7b237e5c4b0db9351d10b3e3fba7",
       "tabbable": null,
       "tooltip": null,
       "value": " 8000/8000 [00:05&lt;00:00, 1559.14 examples/s]"
      }
     },
     "fd070c94aedd4c57a942be665a66527f": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "StyleView",
       "background": null,
       "description_width": "",
       "font_size": null,
       "text_color": null
      }
     }
    },
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
